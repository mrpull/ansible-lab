{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What Is Ansible? \u2301 Ansible Engine is a tool used for automation, provisioning, app deployment, configuration management, and orchestration. Ansible Engine is open source and is sponsored by Red Hat. It is the core of a bigger suite of tools that includes Ansible Tower and Red Hat Automation. More information about Ansible is here . No Agents \u2301 An Ansible Control Node connects via SSH, WinRM, and HTTPS protocols to Managed Nodes or hosts . Unlike SaltStack, Puppet, etc., no agent is required. Ansible is not installed on managed nodes. Firewall changes are often not required since it uses the same protocols used for interactive maintenance. Inventories \u2301 Ansible inventories are (by default) an .INI-style file that contains the names of managed nodes and allows grouping into classes such as webservers , database or environments like production and qa or by location such as us-west1 or us-east1 . There is also a special group all . Inventories can also pull in hosts from AWS, VMware, or scripts. Ad Hoc (and Parallel) Task Execution \u2301 Ansible can be used to run one-off commands on one or many managed nodes (e.g. check free disk space with df -h ) or to execute one of Ansible's hundreds of built in modules (e.g. ansible -m user -a \"user=joedoe state=absent\" ) Modules \u2301 Ansible Modules are pieces of code (usually written in Python or PowerShell) used to accomplish a particular function like configure a user account, install a package, or verify a config file is in compliance. Hundreds of modules are included with Ansible. Modules are also fairly easily written to add functionality to Ansible. The directory of standard modules is here . Tasks \u2301 Tasks are a unit of work. A task may be a single one off command or many tasks can be combined into a sequence of work to configure a system or automate a deployment. Playbooks \u2301 Playbooks are Ansible\u2019s configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process. Roles \u2301 Roles are a way to organize and compartmentalize related Ansible content including tasks, templates, and files. They are used to make reusable components or building blocks. A \"common\" role could be a collection of settings used by all servers and a more specific role could be used to configure a custom application.","title":"Introduction"},{"location":"#what-is-ansible","text":"Ansible Engine is a tool used for automation, provisioning, app deployment, configuration management, and orchestration. Ansible Engine is open source and is sponsored by Red Hat. It is the core of a bigger suite of tools that includes Ansible Tower and Red Hat Automation. More information about Ansible is here .","title":"What Is Ansible?"},{"location":"#no-agents","text":"An Ansible Control Node connects via SSH, WinRM, and HTTPS protocols to Managed Nodes or hosts . Unlike SaltStack, Puppet, etc., no agent is required. Ansible is not installed on managed nodes. Firewall changes are often not required since it uses the same protocols used for interactive maintenance.","title":"No Agents"},{"location":"#inventories","text":"Ansible inventories are (by default) an .INI-style file that contains the names of managed nodes and allows grouping into classes such as webservers , database or environments like production and qa or by location such as us-west1 or us-east1 . There is also a special group all . Inventories can also pull in hosts from AWS, VMware, or scripts.","title":"Inventories"},{"location":"#ad-hoc-and-parallel-task-execution","text":"Ansible can be used to run one-off commands on one or many managed nodes (e.g. check free disk space with df -h ) or to execute one of Ansible's hundreds of built in modules (e.g. ansible -m user -a \"user=joedoe state=absent\" )","title":"Ad Hoc (and Parallel) Task Execution"},{"location":"#modules","text":"Ansible Modules are pieces of code (usually written in Python or PowerShell) used to accomplish a particular function like configure a user account, install a package, or verify a config file is in compliance. Hundreds of modules are included with Ansible. Modules are also fairly easily written to add functionality to Ansible. The directory of standard modules is here .","title":"Modules"},{"location":"#tasks","text":"Tasks are a unit of work. A task may be a single one off command or many tasks can be combined into a sequence of work to configure a system or automate a deployment.","title":"Tasks"},{"location":"#playbooks","text":"Playbooks are Ansible\u2019s configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process.","title":"Playbooks"},{"location":"#roles","text":"Roles are a way to organize and compartmentalize related Ansible content including tasks, templates, and files. They are used to make reusable components or building blocks. A \"common\" role could be a collection of settings used by all servers and a more specific role could be used to configure a custom application.","title":"Roles"},{"location":"keys/keys/","text":"SSH Keys \u2301 In the previous session, it was necessary to first SSH to the target node, accept its fingerprint, and interactively enter a password before executing the first Ansible command. This is OK for one or two nodes but would be painful when scaling to tens, hundreds, or thousands of nodes. SSH provides the ssh-keyscan tool and key based authentication to solve these issues. ssh-keyscan \u2301 ssh-keyscan is a utility for gathering the public SSH host keys of a number of hosts. It can be used to append entries for each of the lab hosts to the management node's ~/.ssh/known_hosts file as follows: $ ssh-keyscan lb1 web1 web2 >> ~/.ssh/known_hosts ssh-keyscan will print a couple lines of output for each node that it scans. It also adds entries to the known_hosts file. If you are curious, you can run cat ~/.ssh/known_hosts to see the current contents. Now try to execute another ad hoc command to test the host keys for each node in the lab: $ ansible -a \"uptime\" all --ask-pass You'll see the following: Ansible connected to each node via SSH without requiring manually accepting each node's host key. It then ran the uptime command on each node to show load averages. Key based authentication \u2301 SSH can provide password-less logins by using a public/private key pair. The details of key based authentication are outside the scope of this tutorial, but the steps below can be followed to setup basic key based auth. Generate an SSH key pair \u2301 The ssh-keygen command can generate a key pair used for SSH authentication. Your own computer may already have a key pair, but the lab mgmt node needs one created as follows: $ ssh-keygen Accept the default location for the key. It isn't best practice to have an empty passphrase, but leave it empty for this lab. After hitting enter a couple times you should see that a key pair is created followed by an ascii art representation of it like this: ssh-copy-id \u2301 You are ready to copy your key to the server. The ssh-copy-id utility makes this easy. $ ssh-copy-id vagrant@web1 Enter the vagrant user's password ( vagrant ) when prompted. When the key copy is complete it will suggest you try logging in again as follows: ssh 'vagrant@web1' . This should connect you to a vagrant@web1:~$ prompt without having to type in a password. Use the exit command to return to the mgmt node. This is a nice overview of ssh-copy-id and password-less logins. Hello Ansible (Again!) \u2301 Now that password-less login is setup on web1 it is time to try another \"Hello World\": $ ansible -a \"echo 'Hello Ansible (Again)'\" web1 Notice that --ask-pass was not needed. Next: Automating Key Distribution \u2301 Using ssh-copy-id for every node in a large environment is a lot of manual work just to get an automation tool working. Fortunately, Ansible has a module to manage ssh keys. The first section of Lab 1 explains how to use Ansible's authorized_key module.","title":"Keys"},{"location":"keys/keys/#ssh-keys","text":"In the previous session, it was necessary to first SSH to the target node, accept its fingerprint, and interactively enter a password before executing the first Ansible command. This is OK for one or two nodes but would be painful when scaling to tens, hundreds, or thousands of nodes. SSH provides the ssh-keyscan tool and key based authentication to solve these issues.","title":"SSH Keys"},{"location":"keys/keys/#ssh-keyscan","text":"ssh-keyscan is a utility for gathering the public SSH host keys of a number of hosts. It can be used to append entries for each of the lab hosts to the management node's ~/.ssh/known_hosts file as follows: $ ssh-keyscan lb1 web1 web2 >> ~/.ssh/known_hosts ssh-keyscan will print a couple lines of output for each node that it scans. It also adds entries to the known_hosts file. If you are curious, you can run cat ~/.ssh/known_hosts to see the current contents. Now try to execute another ad hoc command to test the host keys for each node in the lab: $ ansible -a \"uptime\" all --ask-pass You'll see the following: Ansible connected to each node via SSH without requiring manually accepting each node's host key. It then ran the uptime command on each node to show load averages.","title":"ssh-keyscan"},{"location":"keys/keys/#key-based-authentication","text":"SSH can provide password-less logins by using a public/private key pair. The details of key based authentication are outside the scope of this tutorial, but the steps below can be followed to setup basic key based auth.","title":"Key based authentication"},{"location":"keys/keys/#generate-an-ssh-key-pair","text":"The ssh-keygen command can generate a key pair used for SSH authentication. Your own computer may already have a key pair, but the lab mgmt node needs one created as follows: $ ssh-keygen Accept the default location for the key. It isn't best practice to have an empty passphrase, but leave it empty for this lab. After hitting enter a couple times you should see that a key pair is created followed by an ascii art representation of it like this:","title":"Generate an SSH key pair"},{"location":"keys/keys/#ssh-copy-id","text":"You are ready to copy your key to the server. The ssh-copy-id utility makes this easy. $ ssh-copy-id vagrant@web1 Enter the vagrant user's password ( vagrant ) when prompted. When the key copy is complete it will suggest you try logging in again as follows: ssh 'vagrant@web1' . This should connect you to a vagrant@web1:~$ prompt without having to type in a password. Use the exit command to return to the mgmt node. This is a nice overview of ssh-copy-id and password-less logins.","title":"ssh-copy-id"},{"location":"keys/keys/#hello-ansible-again","text":"Now that password-less login is setup on web1 it is time to try another \"Hello World\": $ ansible -a \"echo 'Hello Ansible (Again)'\" web1 Notice that --ask-pass was not needed.","title":"Hello Ansible (Again!)"},{"location":"keys/keys/#next-automating-key-distribution","text":"Using ssh-copy-id for every node in a large environment is a lot of manual work just to get an automation tool working. Fortunately, Ansible has a module to manage ssh keys. The first section of Lab 1 explains how to use Ansible's authorized_key module.","title":"Next: Automating Key Distribution"},{"location":"lab-1/lab-1/","text":"authorized_keys \u2301 In a previous section you manually copied an SSH key to a managed node. This does not scale well to hundreds of nodes. The following section gives an early peek at using an Ansible playbook that uses the authorized_keys module to distribute your ssh key to all of the lab nodes. Authorized Keys Playbook \u2301 Ansible playbooks are YAML files that contain Ansible configuration and instructions. YAML is relatively easy to read, but sometimes difficult to write. YAML isn't covered in depth here, but the examples should be easy to reproduce. It is important to note that spacing is critical in YAML. If you are getting error, check the spacing in your playbooks. --- - hosts: all become: yes gather_facts: no remote_user: vagrant tasks: - name: install ssh key authorized_key: user: vagrant key: \"{{ lookup('file', '/home/vagrant/.ssh/id_rsa.pub') }}\" state: present Don't worry about all the details right now. In summary, there is one task that runs the authorized_keys module and ensures that the key is present on all nodes in the inventory. The ansible-playbook command is used to execute playbooks. Note this is different than the ansible command that is used for ad hoc executions. Run the playbook 1-1-ssh-addkey.yml as follows to distribute the key. $ ansible-playbook 1-1-ssh-addkey.yml --ask-pass The output will look something like this: Idempotent \u2301 If I'm going to be Idempotent, I want to look Idempotent Good Ansible modules aim to be Idempotent . This means the module can be run again and again with the same input without changing the outcome. In this case, if we run the same playbook again the recap should indicate that none of the nodes changed. Take note that --ask-pass is no longer needed on subsequent executions of ansible-playbook . Ping \u2301 Ansible provides a ping module to help verify hosts are setup correctly. Run the ping module as follows: $ ansible -m ping all Among the output you should see a \"ping\": \"pong\" response for each node in the inventory. This tool is not like the network tool of the same name. It does not check ICMP connectivity, but rather verifies ability to login and a usable environment is available on the managed node. Experiment with the following ping commands to see if you can guess how the groups in inventory.ini are used: $ ansible -m ping lb $ ansible -m ping web $ ansible -m ping all $ ansible -m ping web1 Package, File, Service \u2301 Those familiar with Puppet, Chef, SaltStack, or other similar tools have probably seen the Package, File, Service pattern. A common scenario would be install HTTPD package, update its config file, and start the service on a new web server. Ansible provides multiple ways to accomplish these goals. We'll first look at ad hoc commands, then playbooks, and maybe roles. Warning! Don't do this (unless you must) \u2301 Ubuntu Linux uses the apt package manager to install and update packages. A sysadmin could use a couple ad hoc Ansible invocations that uses the command module like this to install the NTP package on web1 : # DON'T DO THIS $ ansible -a \"sudo apt-get update\" web1 $ ansible -a \"sudo apt-get install ntp\" web1 This is better than logging into each vm and manually running the commands. You'll probably get several warning messages from Ansible not to use sudo and not to call apt-get directly. Ansible provides modules to do it a better way. Better: Ad Hoc pkg Module \u2301 Ansible provides the apt to manage packages on Linux distributions like Debian and Ubuntu. The yum and more generic package modules are also available for Red Hat and other Linux environments. Try this to install and start the NTP service on web1 $ ansible -m apt -a \"package=ntp state=present\" --become web1 # Then try it a second or third time... does it change anything? $ ansible -m apt -a \"package=ntp state=present\" --become web1 # Start NTP using the service module $ ansible -m service -a \"name=ntp state=started\" --become web1 You may have noticed the commands above had a new argument --become . Without diving into details, it means use escalated privileges to execute the modules. In this case it is similar to using sudo to use root privileges. More details here More Better: Playbooks \u2301 Ansible playbooks are a collection of tasks to be executed in sequence. Playbooks are written YAML and are easy to read. At the start of each play you'll usually see: hosts: - which hosts (or groups) to target become: - whether or not privilege escalation is needed gather_facts - whether or not to collect information about the managed node Playbooks include a list of tasks. Tasks should start with a descriptive name followed by the module and its arguments. A playbook consisting of three tasks (and a handler) to install and configure NTP is below: --- - hosts: all become: yes gather_facts: no tasks: - name: install ntp apt: name: ntp state: present update_cache: yes - name: write our ntp.conf copy: src: /home/vagrant/files/ntp.conf dest: /etc/ntp.conf mode: 0644 owner: root group: root notify: restart ntp - name: start ntp service: name: ntp state: started handlers: - name: restart ntp service: name: ntp state: restarted A couple new concepts are introduced in this playbook. The copy module is used to upload and configure the ntp.conf . The handlers section will kick off tasks that are only necessary when something changed. In this case, when ntp.conf is changed it does a notify on the restart handler to restart the NTP service. Run the playbook as follows: ansible-playbook 1-2-ntp-install.yml On the very first run it will install the package, write the config, and start (or restart) ntp. Then run it again: ansible-playbook 1-2-ntp-install.yml On subsequent executions, it should not change anything or restart the service. This is an example of an idempotent playbook. This is a more desirable pattern than ad hoc commands. Review the following playbook and guess what it will do: --- - hosts: all become: yes gather_facts: no tasks: - name: remove ntp apt: name: ntp state: absent If you need a hint, review the docs for the apt module. Now run the playbook above like this: $ ansible-playbook 1-3-ntp-remove.yml Variables and Templates \u2301 Using playbooks like we have so far makes it easy to perform tasks quickly and repeatably. Ansible is even more powerful when you begin to use variables and templates. With variables and templates, you can use the same playbooks and roles across different environments and substitute specific values on per-environment basis. For example, you could configure DNS settings in /etc/resolv.conf for servers in production, disaster recovery, and development datacenters with the same playbook, but substitute the appropriate IP addresses of the DNS servers for each location. NTP Config Template Example \u2301 A good way to get familiar with variables and templates is to walk through an example. In this (somewhat contrived) example, we have decided that the NTP service on the web servers need to use a specific time server while the load balancer must use a different one. This might be the case when the webservers are in a firewalled segment that doesn't permit access to the internet. Our example template file for the NTP playbook is in templates/ntp.conf.j2 . The .j2 extension indicates it is a Jinja 2 template. Jinja is a template engine used by Ansible (and other Python applications). It is a whole language on its own. Today we'll only use it for variable substitution. It may be helpful to think of Ansible templating is like using the mail merge function of an office suite. It takes values from a database and merges them into a templated file. Only one variable is in templates/ntp.conf.j2 : # ...lines above filegen clockstats file clockstats type day enable server {{ ntp_server }} restrict -4 default kod notrap nomodify nopeer noquery # ... lines below This template contains typical ntp.conf values and looks very similar to the file that was used by the copy module in the earlier playbook except the line server {{ ntp_server }} . The template module will replace the section with double curly braces (or mustaches) with a variable. Variables can be defined in several different locations including the inventory, playbooks, or \"vars\" files. In our example, we will use the group_vars directory to define a different value of the variable to be used for each inventory group. The group_vars directory contains a file for each group (and optionally a file for the special all group). Ansible determines the values for variables in templates before deploying the rendered files to the servers. The group_vars directory looks like this: group_vars \u251c\u2500\u2500 lb \u2514\u2500\u2500 web The lb and web files each contains a key/value pair for the {{ ntp_server }} variable. group_vars files can contain multiple variables. Variable can also be YAML dictionaries. See the Using Variables documentation for many more examples. Now, run the updated playbook to use the template: $ ansible-playbook 1-4-ntp-template.yml Try the following ad hoc ansible command to use the command module and grep to inspect the server line /etc/ntp.conf on each server in the inventory: ansible -a \"grep server /etc/ntp.conf\" all We used the same playbook and the template module to configure environment specific config files. The next lab covers using playbooks to configure a semi-realist scenario of multiple applications on multiple servers.","title":"Lab 1"},{"location":"lab-1/lab-1/#authorized_keys","text":"In a previous section you manually copied an SSH key to a managed node. This does not scale well to hundreds of nodes. The following section gives an early peek at using an Ansible playbook that uses the authorized_keys module to distribute your ssh key to all of the lab nodes.","title":"authorized_keys"},{"location":"lab-1/lab-1/#authorized-keys-playbook","text":"Ansible playbooks are YAML files that contain Ansible configuration and instructions. YAML is relatively easy to read, but sometimes difficult to write. YAML isn't covered in depth here, but the examples should be easy to reproduce. It is important to note that spacing is critical in YAML. If you are getting error, check the spacing in your playbooks. --- - hosts: all become: yes gather_facts: no remote_user: vagrant tasks: - name: install ssh key authorized_key: user: vagrant key: \"{{ lookup('file', '/home/vagrant/.ssh/id_rsa.pub') }}\" state: present Don't worry about all the details right now. In summary, there is one task that runs the authorized_keys module and ensures that the key is present on all nodes in the inventory. The ansible-playbook command is used to execute playbooks. Note this is different than the ansible command that is used for ad hoc executions. Run the playbook 1-1-ssh-addkey.yml as follows to distribute the key. $ ansible-playbook 1-1-ssh-addkey.yml --ask-pass The output will look something like this:","title":"Authorized Keys Playbook"},{"location":"lab-1/lab-1/#idempotent","text":"If I'm going to be Idempotent, I want to look Idempotent Good Ansible modules aim to be Idempotent . This means the module can be run again and again with the same input without changing the outcome. In this case, if we run the same playbook again the recap should indicate that none of the nodes changed. Take note that --ask-pass is no longer needed on subsequent executions of ansible-playbook .","title":"Idempotent"},{"location":"lab-1/lab-1/#ping","text":"Ansible provides a ping module to help verify hosts are setup correctly. Run the ping module as follows: $ ansible -m ping all Among the output you should see a \"ping\": \"pong\" response for each node in the inventory. This tool is not like the network tool of the same name. It does not check ICMP connectivity, but rather verifies ability to login and a usable environment is available on the managed node. Experiment with the following ping commands to see if you can guess how the groups in inventory.ini are used: $ ansible -m ping lb $ ansible -m ping web $ ansible -m ping all $ ansible -m ping web1","title":"Ping"},{"location":"lab-1/lab-1/#package-file-service","text":"Those familiar with Puppet, Chef, SaltStack, or other similar tools have probably seen the Package, File, Service pattern. A common scenario would be install HTTPD package, update its config file, and start the service on a new web server. Ansible provides multiple ways to accomplish these goals. We'll first look at ad hoc commands, then playbooks, and maybe roles.","title":"Package, File, Service"},{"location":"lab-1/lab-1/#warning-dont-do-this-unless-you-must","text":"Ubuntu Linux uses the apt package manager to install and update packages. A sysadmin could use a couple ad hoc Ansible invocations that uses the command module like this to install the NTP package on web1 : # DON'T DO THIS $ ansible -a \"sudo apt-get update\" web1 $ ansible -a \"sudo apt-get install ntp\" web1 This is better than logging into each vm and manually running the commands. You'll probably get several warning messages from Ansible not to use sudo and not to call apt-get directly. Ansible provides modules to do it a better way.","title":"Warning!  Don't do this (unless you must)"},{"location":"lab-1/lab-1/#better-ad-hoc-pkg-module","text":"Ansible provides the apt to manage packages on Linux distributions like Debian and Ubuntu. The yum and more generic package modules are also available for Red Hat and other Linux environments. Try this to install and start the NTP service on web1 $ ansible -m apt -a \"package=ntp state=present\" --become web1 # Then try it a second or third time... does it change anything? $ ansible -m apt -a \"package=ntp state=present\" --become web1 # Start NTP using the service module $ ansible -m service -a \"name=ntp state=started\" --become web1 You may have noticed the commands above had a new argument --become . Without diving into details, it means use escalated privileges to execute the modules. In this case it is similar to using sudo to use root privileges. More details here","title":"Better: Ad Hoc pkg Module"},{"location":"lab-1/lab-1/#more-better-playbooks","text":"Ansible playbooks are a collection of tasks to be executed in sequence. Playbooks are written YAML and are easy to read. At the start of each play you'll usually see: hosts: - which hosts (or groups) to target become: - whether or not privilege escalation is needed gather_facts - whether or not to collect information about the managed node Playbooks include a list of tasks. Tasks should start with a descriptive name followed by the module and its arguments. A playbook consisting of three tasks (and a handler) to install and configure NTP is below: --- - hosts: all become: yes gather_facts: no tasks: - name: install ntp apt: name: ntp state: present update_cache: yes - name: write our ntp.conf copy: src: /home/vagrant/files/ntp.conf dest: /etc/ntp.conf mode: 0644 owner: root group: root notify: restart ntp - name: start ntp service: name: ntp state: started handlers: - name: restart ntp service: name: ntp state: restarted A couple new concepts are introduced in this playbook. The copy module is used to upload and configure the ntp.conf . The handlers section will kick off tasks that are only necessary when something changed. In this case, when ntp.conf is changed it does a notify on the restart handler to restart the NTP service. Run the playbook as follows: ansible-playbook 1-2-ntp-install.yml On the very first run it will install the package, write the config, and start (or restart) ntp. Then run it again: ansible-playbook 1-2-ntp-install.yml On subsequent executions, it should not change anything or restart the service. This is an example of an idempotent playbook. This is a more desirable pattern than ad hoc commands. Review the following playbook and guess what it will do: --- - hosts: all become: yes gather_facts: no tasks: - name: remove ntp apt: name: ntp state: absent If you need a hint, review the docs for the apt module. Now run the playbook above like this: $ ansible-playbook 1-3-ntp-remove.yml","title":"More Better: Playbooks"},{"location":"lab-1/lab-1/#variables-and-templates","text":"Using playbooks like we have so far makes it easy to perform tasks quickly and repeatably. Ansible is even more powerful when you begin to use variables and templates. With variables and templates, you can use the same playbooks and roles across different environments and substitute specific values on per-environment basis. For example, you could configure DNS settings in /etc/resolv.conf for servers in production, disaster recovery, and development datacenters with the same playbook, but substitute the appropriate IP addresses of the DNS servers for each location.","title":"Variables and Templates"},{"location":"lab-1/lab-1/#ntp-config-template-example","text":"A good way to get familiar with variables and templates is to walk through an example. In this (somewhat contrived) example, we have decided that the NTP service on the web servers need to use a specific time server while the load balancer must use a different one. This might be the case when the webservers are in a firewalled segment that doesn't permit access to the internet. Our example template file for the NTP playbook is in templates/ntp.conf.j2 . The .j2 extension indicates it is a Jinja 2 template. Jinja is a template engine used by Ansible (and other Python applications). It is a whole language on its own. Today we'll only use it for variable substitution. It may be helpful to think of Ansible templating is like using the mail merge function of an office suite. It takes values from a database and merges them into a templated file. Only one variable is in templates/ntp.conf.j2 : # ...lines above filegen clockstats file clockstats type day enable server {{ ntp_server }} restrict -4 default kod notrap nomodify nopeer noquery # ... lines below This template contains typical ntp.conf values and looks very similar to the file that was used by the copy module in the earlier playbook except the line server {{ ntp_server }} . The template module will replace the section with double curly braces (or mustaches) with a variable. Variables can be defined in several different locations including the inventory, playbooks, or \"vars\" files. In our example, we will use the group_vars directory to define a different value of the variable to be used for each inventory group. The group_vars directory contains a file for each group (and optionally a file for the special all group). Ansible determines the values for variables in templates before deploying the rendered files to the servers. The group_vars directory looks like this: group_vars \u251c\u2500\u2500 lb \u2514\u2500\u2500 web The lb and web files each contains a key/value pair for the {{ ntp_server }} variable. group_vars files can contain multiple variables. Variable can also be YAML dictionaries. See the Using Variables documentation for many more examples. Now, run the updated playbook to use the template: $ ansible-playbook 1-4-ntp-template.yml Try the following ad hoc ansible command to use the command module and grep to inspect the server line /etc/ntp.conf on each server in the inventory: ansible -a \"grep server /etc/ntp.conf\" all We used the same playbook and the template module to configure environment specific config files. The next lab covers using playbooks to configure a semi-realist scenario of multiple applications on multiple servers.","title":"NTP Config Template Example"},{"location":"lab-2/lab-2/","text":"Load Balancer and 2 (or more) Web Servers \u2301 The end result of this lab will be an automatically deployed and configured HAProxy load balancer with a group of web servers. Playbook with multiple plays \u2301 Review the contents of 2-1-site.yml . It contains multiple plays that use modules that have already been covered and also introduces a couple new ones. Common \u2301 The first play targets all nodes in the inventory. It uses the apt module to ensure the git package is installed on all nodes. These settings are common to all managed nodes. Web \u2301 The next play targets only members of the web group in the inventory. It uses the apt module to install the NGINX webserver, template to create the NGINX config file and page content. If needed, it will use a handler to restart NGINX. Load balancer \u2301 The final play targets only the load balancer group. This play employs some new tricks: The apt module now uses with_items . You can think of with_items as looping over the list and running the apt module for each item in the list. It uses the variable syntax to substitute each package name. - name: install haproxy and socat apt: name: \"{{ item }}\" state: present with_items: - haproxy - socat The next play uses the lineinfile . lineinfile ensures a particular line is in a file, or replace an existing line using a regular expression. This is primarily useful when you want to change a single line in a file only. The template module is preferable in cases where control of the contents of the entire file is important. The logic is \"find a line that starts with ENABLED\" and replace it with \"ENABLED=1\". * - name: enable haproxy lineinfile: dest: /etc/default/haproxy regexp: \"^ENABLED\" line: \"ENABLED=1\" notify: restart haproxy The templates directory contains .j2 (Jinja2) files that are merged before playbook execution. The templates are mostly stock configuration files plus ansible variables and a little bit of Ansible magic. The template/default-site.j2 template is used to create the NGINX site configuration. It begins by inserting a comment that the file is managed by Ansible with the {{ ansible_managed }} variable. You can customize this message to warn sysadmins that changes made by hand will be overwritten by the next Ansible run. The NGINX configuration should include the server's hostname. Rather than hardcoding the hostname in the template, the Ansible Fact {{ ansible_hostname }} dynamically inserts the hostname on a per-host basis. This is a good pattern for making playbooks reusable. # {{ ansible_managed }} server { listen 80; server_name {{ ansible_hostname }}; root /usr/share/nginx/html; index index.html index.htm; location / { try_files $uri $uri/ =404; } error_page 404 /404.html; error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } The other templates have employ similar methods of variable and fact substitution. A more advanced use of templating occurs in the haproxy.cfg.j2 template. The backend app section of the configuration uses Jinja loops and Ansible facts to render the config. This will later allow adding new hosts to the web group with no need to change the template. Jinja loops are an advanced template topic for future learning. See below for an example of the template and rendered config file side by side. Play Time... \u2301 Run the playbook to configure the environment. $ ansible-playbook 2-1-site.yml If all goes well, your play recap will indicate success. If you run the playbook a second time, the recap should indicate no changes. Behind the scenes, Vagrant has forwarded a port to the HAProxy virtual machine. Test it out by browsing to http://localhost:8080 . Refresh the browser several times to see if traffic is being served from both servers. Experiment \u2301 Now try to increase the number of web servers. Update the Vagrantfile to 6 servers (or 3 or 4 if your computer is low on memory, but not more than 9). Change the 2 in (1..2).each do |i| to 6 like the example. The Vagrantfile syntax isn't related to Ansible and doesn't need to be understood. Launch the new VM's: $ vagrant up After the new servers are booted, you'll need to update the inventory to include them (in this case remove the #'s from each relevant line in inventory.ini). Also you'll need to make sure password-less logins are working. The Keys lab can guide you if you need help. Run the playbook again. Refresh the browser. Does it change? Experiment with using templates to deploy changes to the contents of the web page. Try ad hoc commands to stop the service on one or two nodes. What happens when you refresh the web page? What does ansible -a \"cat /etc/haproxy/haproxy.cfg\" lb1 do? Lab 3 will discuss breaking down larger playbooks into more modular roles.","title":"Lab 2"},{"location":"lab-2/lab-2/#load-balancer-and-2-or-more-web-servers","text":"The end result of this lab will be an automatically deployed and configured HAProxy load balancer with a group of web servers.","title":"Load Balancer and 2 (or more) Web Servers"},{"location":"lab-2/lab-2/#playbook-with-multiple-plays","text":"Review the contents of 2-1-site.yml . It contains multiple plays that use modules that have already been covered and also introduces a couple new ones.","title":"Playbook with multiple plays"},{"location":"lab-2/lab-2/#common","text":"The first play targets all nodes in the inventory. It uses the apt module to ensure the git package is installed on all nodes. These settings are common to all managed nodes.","title":"Common"},{"location":"lab-2/lab-2/#web","text":"The next play targets only members of the web group in the inventory. It uses the apt module to install the NGINX webserver, template to create the NGINX config file and page content. If needed, it will use a handler to restart NGINX.","title":"Web"},{"location":"lab-2/lab-2/#load-balancer","text":"The final play targets only the load balancer group. This play employs some new tricks: The apt module now uses with_items . You can think of with_items as looping over the list and running the apt module for each item in the list. It uses the variable syntax to substitute each package name. - name: install haproxy and socat apt: name: \"{{ item }}\" state: present with_items: - haproxy - socat The next play uses the lineinfile . lineinfile ensures a particular line is in a file, or replace an existing line using a regular expression. This is primarily useful when you want to change a single line in a file only. The template module is preferable in cases where control of the contents of the entire file is important. The logic is \"find a line that starts with ENABLED\" and replace it with \"ENABLED=1\". * - name: enable haproxy lineinfile: dest: /etc/default/haproxy regexp: \"^ENABLED\" line: \"ENABLED=1\" notify: restart haproxy The templates directory contains .j2 (Jinja2) files that are merged before playbook execution. The templates are mostly stock configuration files plus ansible variables and a little bit of Ansible magic. The template/default-site.j2 template is used to create the NGINX site configuration. It begins by inserting a comment that the file is managed by Ansible with the {{ ansible_managed }} variable. You can customize this message to warn sysadmins that changes made by hand will be overwritten by the next Ansible run. The NGINX configuration should include the server's hostname. Rather than hardcoding the hostname in the template, the Ansible Fact {{ ansible_hostname }} dynamically inserts the hostname on a per-host basis. This is a good pattern for making playbooks reusable. # {{ ansible_managed }} server { listen 80; server_name {{ ansible_hostname }}; root /usr/share/nginx/html; index index.html index.htm; location / { try_files $uri $uri/ =404; } error_page 404 /404.html; error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } The other templates have employ similar methods of variable and fact substitution. A more advanced use of templating occurs in the haproxy.cfg.j2 template. The backend app section of the configuration uses Jinja loops and Ansible facts to render the config. This will later allow adding new hosts to the web group with no need to change the template. Jinja loops are an advanced template topic for future learning. See below for an example of the template and rendered config file side by side.","title":"Load balancer"},{"location":"lab-2/lab-2/#play-time","text":"Run the playbook to configure the environment. $ ansible-playbook 2-1-site.yml If all goes well, your play recap will indicate success. If you run the playbook a second time, the recap should indicate no changes. Behind the scenes, Vagrant has forwarded a port to the HAProxy virtual machine. Test it out by browsing to http://localhost:8080 . Refresh the browser several times to see if traffic is being served from both servers.","title":"Play Time..."},{"location":"lab-2/lab-2/#experiment","text":"Now try to increase the number of web servers. Update the Vagrantfile to 6 servers (or 3 or 4 if your computer is low on memory, but not more than 9). Change the 2 in (1..2).each do |i| to 6 like the example. The Vagrantfile syntax isn't related to Ansible and doesn't need to be understood. Launch the new VM's: $ vagrant up After the new servers are booted, you'll need to update the inventory to include them (in this case remove the #'s from each relevant line in inventory.ini). Also you'll need to make sure password-less logins are working. The Keys lab can guide you if you need help. Run the playbook again. Refresh the browser. Does it change? Experiment with using templates to deploy changes to the contents of the web page. Try ad hoc commands to stop the service on one or two nodes. What happens when you refresh the web page? What does ansible -a \"cat /etc/haproxy/haproxy.cfg\" lb1 do? Lab 3 will discuss breaking down larger playbooks into more modular roles.","title":"Experiment"},{"location":"lab-3/lab-3/","text":"Advanced Role Playing \u2301 Ansible roles allow you to remove bulky configuration out of the playbook, and into a folder structure. The entire goal of this, is to make things modular, so that you can share and reuse roles, along with not having playbooks that are thousands of lines long. A role might be created to manage a particular application on a particular kind of server (e.g. the SMTP service on mail servers) or to deploy a common application or setting to all servers. Role creation may be assigned to developers on different teams (database vs. application serever). Separation of responsibilities in roles also allows Ansible playbooks to be maintained independently based on skillsets or knowledge domains. Role syntax is similar to playbooks, but the pieces are broken into a directory and file structure. Roles expect files to be in certain directories. Roles must include at least one of these directories, however it is perfectly fine to exclude any which are not used. When in use, each directory must contain a main.yml file, which contains the relevant content: tasks - contains the main list of tasks to be executed by the role. handlers - contains handlers, which may be used by this role or even anywhere outside this role. defaults - default variables for the role. vars - other variables for the role. files - contains files which can be deployed via this role. templates - contains templates which can be deployed via this role. meta - defines some meta data for this role. This lab is only a high overview of roles. More details can be found in the Ansible documentation . The Web Role \u2301 Reviewing existing examples is a good way to learn roles. The web role only contains handlers, tasks, and templates directories. Tasks and handlers directories contain a main.yml file that is the entry point for each folder. More complex roles will break down tasks into smaller chunks and use the main.yml file to import_tasks from other files. The web role in this lab is structured like this: web \u251c\u2500\u2500 handlers \u2502 \u2514\u2500\u2500 main.yml \u251c\u2500\u2500 tasks \u2502 \u2514\u2500\u2500 main.yml \u2514\u2500\u2500 templates \u251c\u2500\u2500 default-site.j2 \u251c\u2500\u2500 index.html.j2 \u2514\u2500\u2500 nginx.conf.j2 The roles/web/tasks/main.yml file contains a YAML formatted list of tasks. Its contents will look familiar. It does not include hosts: , become: , gather_facts: , etc. that you have seen in playbooks. --- - name: install nginx apt: name: nginx state: present - name: write our nginx.conf template: src: nginx.conf.j2 dest: /etc/nginx/nginx.conf notify: restart nginx - name: write our /etc/nginx/sites-available/default template: src: default-site.j2 dest: /etc/nginx/sites-available/default notify: restart nginx - name: deploy website content template: src: index.html.j2 dest: /usr/share/nginx/html/index.html Playbooks are still used to execute roles. The playbook 3-role-site.yml is similar to 2-1-site.yml . It still executes three plays (each for common, web, and load balancer) but has a roles: section instead of tasks . --- # common - hosts: all become: yes gather_facts: no roles: - common # web - hosts: web become: yes roles: - web # lb - hosts: lb become: yes roles: - lb Run the playbook to see if any changes are observed. $ ansible-playbook 3-role-site.yml While the responsibilities of different plays are now broken into roles, the end result should be the same. Reusable Roles \u2301 Making roles reusable should be a goal. Consideration should be taken if the role will be used in Red Hat Linux vs. Ubuntu or even Windows. Roles provide flexibility to accommodate many types of reuse. Some interesting examples of more complex roles are: Install Docker: https://github.com/geerlingguy/ansible-role-docker Install New Relic Infrastructure: https://github.com/newrelic/infrastructure-agent-ansible Ansible Galaxy is a site for sharing roles you create or consuming ready made roles. Just the beginning \u2301 This lab only scratches the surface of what can be done with Ansible. However, you should be comfortable with the basic concepts and be able to experiment further. Lab4 outlines (but doesn't deep dive into) a rolling update of the content of the web servers. More resources can be found in the More section of this lab. If you are finished, shut down the Vagrant environment like this: # Stop VM's $ vagrant halt # Stop and remove from disk $ vagrant destroy You can do a vagrant up to start up a halted environment or recreate a destroyed environment.","title":"Lab 3"},{"location":"lab-3/lab-3/#advanced-role-playing","text":"Ansible roles allow you to remove bulky configuration out of the playbook, and into a folder structure. The entire goal of this, is to make things modular, so that you can share and reuse roles, along with not having playbooks that are thousands of lines long. A role might be created to manage a particular application on a particular kind of server (e.g. the SMTP service on mail servers) or to deploy a common application or setting to all servers. Role creation may be assigned to developers on different teams (database vs. application serever). Separation of responsibilities in roles also allows Ansible playbooks to be maintained independently based on skillsets or knowledge domains. Role syntax is similar to playbooks, but the pieces are broken into a directory and file structure. Roles expect files to be in certain directories. Roles must include at least one of these directories, however it is perfectly fine to exclude any which are not used. When in use, each directory must contain a main.yml file, which contains the relevant content: tasks - contains the main list of tasks to be executed by the role. handlers - contains handlers, which may be used by this role or even anywhere outside this role. defaults - default variables for the role. vars - other variables for the role. files - contains files which can be deployed via this role. templates - contains templates which can be deployed via this role. meta - defines some meta data for this role. This lab is only a high overview of roles. More details can be found in the Ansible documentation .","title":"Advanced Role Playing"},{"location":"lab-3/lab-3/#the-web-role","text":"Reviewing existing examples is a good way to learn roles. The web role only contains handlers, tasks, and templates directories. Tasks and handlers directories contain a main.yml file that is the entry point for each folder. More complex roles will break down tasks into smaller chunks and use the main.yml file to import_tasks from other files. The web role in this lab is structured like this: web \u251c\u2500\u2500 handlers \u2502 \u2514\u2500\u2500 main.yml \u251c\u2500\u2500 tasks \u2502 \u2514\u2500\u2500 main.yml \u2514\u2500\u2500 templates \u251c\u2500\u2500 default-site.j2 \u251c\u2500\u2500 index.html.j2 \u2514\u2500\u2500 nginx.conf.j2 The roles/web/tasks/main.yml file contains a YAML formatted list of tasks. Its contents will look familiar. It does not include hosts: , become: , gather_facts: , etc. that you have seen in playbooks. --- - name: install nginx apt: name: nginx state: present - name: write our nginx.conf template: src: nginx.conf.j2 dest: /etc/nginx/nginx.conf notify: restart nginx - name: write our /etc/nginx/sites-available/default template: src: default-site.j2 dest: /etc/nginx/sites-available/default notify: restart nginx - name: deploy website content template: src: index.html.j2 dest: /usr/share/nginx/html/index.html Playbooks are still used to execute roles. The playbook 3-role-site.yml is similar to 2-1-site.yml . It still executes three plays (each for common, web, and load balancer) but has a roles: section instead of tasks . --- # common - hosts: all become: yes gather_facts: no roles: - common # web - hosts: web become: yes roles: - web # lb - hosts: lb become: yes roles: - lb Run the playbook to see if any changes are observed. $ ansible-playbook 3-role-site.yml While the responsibilities of different plays are now broken into roles, the end result should be the same.","title":"The Web Role"},{"location":"lab-3/lab-3/#reusable-roles","text":"Making roles reusable should be a goal. Consideration should be taken if the role will be used in Red Hat Linux vs. Ubuntu or even Windows. Roles provide flexibility to accommodate many types of reuse. Some interesting examples of more complex roles are: Install Docker: https://github.com/geerlingguy/ansible-role-docker Install New Relic Infrastructure: https://github.com/newrelic/infrastructure-agent-ansible Ansible Galaxy is a site for sharing roles you create or consuming ready made roles.","title":"Reusable Roles"},{"location":"lab-3/lab-3/#just-the-beginning","text":"This lab only scratches the surface of what can be done with Ansible. However, you should be comfortable with the basic concepts and be able to experiment further. Lab4 outlines (but doesn't deep dive into) a rolling update of the content of the web servers. More resources can be found in the More section of this lab. If you are finished, shut down the Vagrant environment like this: # Stop VM's $ vagrant halt # Stop and remove from disk $ vagrant destroy You can do a vagrant up to start up a halted environment or recreate a destroyed environment.","title":"Just the beginning"},{"location":"lab-4/lab-4/","text":"Are you ready? \u2301 If you haven't already, scale your Vagrant web farm to 6 virtual machines. Make sure all nodes are configured for password-less logins Run ansible -m ping all to test ansible connectivity Run ansible-playbook 3-role-site.yml to get environment to known good configuration Browse to http://localhost:8080 and refresh a few times Rolling Updates \u2301 Users (and businesses) expect nearly 100% uptime of websites. A rolling update, or updating each server one at a time, can provide a zero-downtime deployment. Ansible can orchestrate these deployments. Serial Tasks \u2301 By default, Ansible runs tasks in parallel across hosts (up to the number in the forks setting in the [default] section of ansible.cfg ). In our rolling update scenario, we only want one web server at a time removed from the pool for updating. The serial: 1 setting of the web play will limit execution to one server at a time until all hosts in the group have been processed. Skim through the playbook below to see the tasks. See comments inline for new concepts. This playbook isn't appropriate for production, but could be good for future inspiration. The shell commands that disable and enable servers in the load balancer pools are ugly, but should do the trick. --- # common - hosts: all become: yes gather_facts: no tasks: - name: install git apt: name: git state: present update_cache: yes # web - hosts: web become: yes vars: app_version: release-0.01 # Limit execution to one node at a time serial: 1 # Tasks to execute before roles or main tasks section. pre_tasks: - name: disable server in haproxy shell: echo \"disable server ansible-lab/{{ inventory_hostname }}\" | socat stdio /var/lib/haproxy/stats delegate_to: \"{{ item }}\" with_items: \"{{ groups.lb }}\" tasks: - name: install nginx apt: name: nginx state: present - name: write our nginx.conf template: src: templates/nginx.conf.j2 dest: /etc/nginx/nginx.conf notify: restart nginx - name: write our /etc/nginx/sites-available/default template: src: templates/default-site.j2 dest: /etc/nginx/sites-available/default notify: restart nginx - name: clean existing website content file: path: /usr/share/nginx/html/ state: absent # The git module will pull requested version from git - name: deploy website content git: repo: https://github.com/mrpull/episode-47.git dest: /usr/share/nginx/html/ version: \"{{ app_version }}\" handlers: - name: restart nginx service: name: nginx state: restarted # Tasks to execute after roles or main tasks section. post_tasks: - name: enable server in haproxy shell: echo \"enable server ansible-lab/{{ inventory_hostname }}\" | socat stdio /var/lib/haproxy/stats delegate_to: \"{{ item }}\" with_items: \"{{ groups.lb }}\" # lb - hosts: lb become: yes tasks: - name: Download and install haproxy and socat apt: name: \"{{ item }}\" state: present with_items: - haproxy - socat - name: Enable HAProxy lineinfile: path: /etc/default/haproxy regexp: \"^ENABLED\" line: \"ENABLED=1\" notify: restart haproxy - name: Configure the haproxy cnf file with hosts template: src: templates/haproxy.cfg.j2 dest: /etc/haproxy/haproxy.cfg notify: restart haproxy handlers: - name: restart haproxy service: name: haproxy state: restarted Run the playbook while frequently refreshing your browser on http://localhost:8080 . You may also want to browse to and refresh http://localhost:8080/haproxy?stats to see how the HAProxy changes throughout the playbook. $ ansible-playbook 4-rolling.yml -e \"app_version=release-0.01\" # Run again with a new version $ ansible-playbook 4-rolling.yml -e \"app_version=release-0.02\" # Run again with another new version $ ansible-playbook 4-rolling.yml -e \"app_version=release-0.03\" A parallel deployment may be faster, but the rolling update helps avoid user interruption. Go Forth And Automate \u2301 If you are finished, shut down the Vagrant environment like this: # Stop VM's $ vagrant halt # Stop and remove from disk $ vagrant destroy You can do a vagrant up to start up a halted environment or recreate a fresh environment after a destroy. More resources can be found in the More section of this lab.","title":"Lab 4"},{"location":"lab-4/lab-4/#are-you-ready","text":"If you haven't already, scale your Vagrant web farm to 6 virtual machines. Make sure all nodes are configured for password-less logins Run ansible -m ping all to test ansible connectivity Run ansible-playbook 3-role-site.yml to get environment to known good configuration Browse to http://localhost:8080 and refresh a few times","title":"Are you ready?"},{"location":"lab-4/lab-4/#rolling-updates","text":"Users (and businesses) expect nearly 100% uptime of websites. A rolling update, or updating each server one at a time, can provide a zero-downtime deployment. Ansible can orchestrate these deployments.","title":"Rolling Updates"},{"location":"lab-4/lab-4/#serial-tasks","text":"By default, Ansible runs tasks in parallel across hosts (up to the number in the forks setting in the [default] section of ansible.cfg ). In our rolling update scenario, we only want one web server at a time removed from the pool for updating. The serial: 1 setting of the web play will limit execution to one server at a time until all hosts in the group have been processed. Skim through the playbook below to see the tasks. See comments inline for new concepts. This playbook isn't appropriate for production, but could be good for future inspiration. The shell commands that disable and enable servers in the load balancer pools are ugly, but should do the trick. --- # common - hosts: all become: yes gather_facts: no tasks: - name: install git apt: name: git state: present update_cache: yes # web - hosts: web become: yes vars: app_version: release-0.01 # Limit execution to one node at a time serial: 1 # Tasks to execute before roles or main tasks section. pre_tasks: - name: disable server in haproxy shell: echo \"disable server ansible-lab/{{ inventory_hostname }}\" | socat stdio /var/lib/haproxy/stats delegate_to: \"{{ item }}\" with_items: \"{{ groups.lb }}\" tasks: - name: install nginx apt: name: nginx state: present - name: write our nginx.conf template: src: templates/nginx.conf.j2 dest: /etc/nginx/nginx.conf notify: restart nginx - name: write our /etc/nginx/sites-available/default template: src: templates/default-site.j2 dest: /etc/nginx/sites-available/default notify: restart nginx - name: clean existing website content file: path: /usr/share/nginx/html/ state: absent # The git module will pull requested version from git - name: deploy website content git: repo: https://github.com/mrpull/episode-47.git dest: /usr/share/nginx/html/ version: \"{{ app_version }}\" handlers: - name: restart nginx service: name: nginx state: restarted # Tasks to execute after roles or main tasks section. post_tasks: - name: enable server in haproxy shell: echo \"enable server ansible-lab/{{ inventory_hostname }}\" | socat stdio /var/lib/haproxy/stats delegate_to: \"{{ item }}\" with_items: \"{{ groups.lb }}\" # lb - hosts: lb become: yes tasks: - name: Download and install haproxy and socat apt: name: \"{{ item }}\" state: present with_items: - haproxy - socat - name: Enable HAProxy lineinfile: path: /etc/default/haproxy regexp: \"^ENABLED\" line: \"ENABLED=1\" notify: restart haproxy - name: Configure the haproxy cnf file with hosts template: src: templates/haproxy.cfg.j2 dest: /etc/haproxy/haproxy.cfg notify: restart haproxy handlers: - name: restart haproxy service: name: haproxy state: restarted Run the playbook while frequently refreshing your browser on http://localhost:8080 . You may also want to browse to and refresh http://localhost:8080/haproxy?stats to see how the HAProxy changes throughout the playbook. $ ansible-playbook 4-rolling.yml -e \"app_version=release-0.01\" # Run again with a new version $ ansible-playbook 4-rolling.yml -e \"app_version=release-0.02\" # Run again with another new version $ ansible-playbook 4-rolling.yml -e \"app_version=release-0.03\" A parallel deployment may be faster, but the rolling update helps avoid user interruption.","title":"Serial Tasks"},{"location":"lab-4/lab-4/#go-forth-and-automate","text":"If you are finished, shut down the Vagrant environment like this: # Stop VM's $ vagrant halt # Stop and remove from disk $ vagrant destroy You can do a vagrant up to start up a halted environment or recreate a fresh environment after a destroy. More resources can be found in the More section of this lab.","title":"Go Forth And Automate"},{"location":"more/more/","text":"Credits \u2301 This lab is based on an excellent series of Ansible tutorials by justin@sysadmincasts.com. Justin's content has not been updated for newer versions of Ansible, but the core concepts remain the same. Syntax has been updated to newer standards and has been tested with Ansible 2.8.5. The original four part series can be found at: https://sysadmincasts.com/episodes/43-19-minutes-with-ansible-part-1-4 https://sysadmincasts.com/episodes/45-learning-ansible-with-vagrant-part-2-4 https://sysadmincasts.com/episodes/46-configuration-management-with-ansible-part-3-4 https://sysadmincasts.com/episodes/47-zero-downtime-deployments-with-ansible-part-4-4 More Resources \u2301 Ansible Documentation Get Started With Ansible How Ansible works Ansible Playbook Best Practices","title":"More"},{"location":"more/more/#credits","text":"This lab is based on an excellent series of Ansible tutorials by justin@sysadmincasts.com. Justin's content has not been updated for newer versions of Ansible, but the core concepts remain the same. Syntax has been updated to newer standards and has been tested with Ansible 2.8.5. The original four part series can be found at: https://sysadmincasts.com/episodes/43-19-minutes-with-ansible-part-1-4 https://sysadmincasts.com/episodes/45-learning-ansible-with-vagrant-part-2-4 https://sysadmincasts.com/episodes/46-configuration-management-with-ansible-part-3-4 https://sysadmincasts.com/episodes/47-zero-downtime-deployments-with-ansible-part-4-4","title":"Credits"},{"location":"more/more/#more-resources","text":"Ansible Documentation Get Started With Ansible How Ansible works Ansible Playbook Best Practices","title":"More Resources"},{"location":"setup/setup/","text":"Overview \u2301 The intent of this lab is to provide a gentle introduction to Ansible and provide an environment for further learning and experimentation. Not all Ansible features are covered, but basics such as ad hoc command execution, playbooks, and templates are introduced. Lab Software (Prerequisites) \u2301 The lab environment consists of Ubuntu virtual machines powered by VirtualBox and managed by Vagrant. Neither VirtualBox or Vagrant are covered in depth. They are used as part of the lab, but full understanding isn't required. Each of the following should be installed prior to the lab. Setup of prerequisites will take around 20 minutes: Git and SSH are used for downloading example code and connecting to virtual machines. Many Linux distributions and macOS provide these tools out of the box. The Windows Subsystem for Linux and Git for Windows are both good options to use Git and SSH in Windows. Git for Windows is recommended. VirtualBox is an open source virtualization product. It runs on Windows, Linux, and Mac and supports most common guest operating systems. You may need to run the installer as administrator to avoid an installation error. Vagrant is a tool for building and managing virtual machine environments. A Vagrantfile provides a configuration that builds and provisions the multiple VM's in this lab. Understanding of Vagrant isn't necessary, but being familiar with a few basic commands is helpful: vagrant up : starts and provisions the Vagrant environment vagrant ssh <hostname> : connect to virtual machine with SSH vagrant halt : stops virtual machine(s) vagrant destroy : terminate and destroy virtual machines varant help : get Vagrant help Getting Started: \u2301 Open shell and change to an appropriate directory. Clone the git repository and cd into it: $ cd ~/Downloads $ git clone <PLACEHOLDER> $ cd ansible-lab Use your favorite text editor, less, or more to inspect the Vagrantfile and bootstrap-mgmt.sh files. The Vagrantfile (initially) defines four virtual machines. web1 and web2 will be configured as web servers. lb1 will be configured as an HAProxy software load balancer to distribute traffic to each webserver. A control node called mgmt will be configured (or bootstrapped) by Vagrant to automatically preinstall Ansible, copy lab examples, and populate its /etc/hosts file. It it not necessary to understand the Vagrant files in order to do the Ansible lab. Start the Virtual Machines \u2301 From the ansible-lab directory, use Vagrant to start the four virtual machines. $ vagrant up The vagrant up command will take several minutes (even on a fast internet connection) as it downloads virtual machines, boots them, and configures the virtual environment. Now is a good time to take a break. Connect to the mgmt server like this: $ vagrant ssh mgmt Hello Ansible! \u2301 If all went well, you should be at a shell prompt inside the mgmt virtual machine that looks like vagrant@mgmt:~$ . Once ssh'ed into the mgmt node. Verify that Ansible has been automatically been installed. $ ansible --version Since Ansible will be using SSH to connect to our managed nodes, make sure you can SSH to web1 . $ ssh web1 You should receive a warning similar to the following: If you know SSH, you'll recognize that the SSH client doesn't recognize the fingerprint of the SSH server. Since it is the first time connecting to this node it is to be expected. Answer yes to add web1 to the list of known hosts. The default password for the vagrant user on the lab machines is vagrant . Enter the password and you should be at a vagrant@web1:~$ prompt inside the web1 virtual machine. Use the exit command to return to the mgmt machine. Now that you can SSH to web1 you are ready for the first Ansible command. As tradition dictates, our first command will print Hello Ansible . You will be asked for the SSH password after you run the following: $ ansible -a \"echo 'Hello Ansible'\" web1 --ask-pass This is an ad hoc Ansible execution that targets the web1 node and implicitly uses the command module to execute echo 'Hello Ansible . --ask-pass (which is abbreviated -k ) tells Ansible you want to provide a password interactively. If it works, you'll see: Manually accepting host keys and typing passwords interactively is no fun. You'll see in the next session how to make it less painful. More resources \u2301 The Ansible documentation has a nice introduction to ad hoc commands here .","title":"Setup"},{"location":"setup/setup/#overview","text":"The intent of this lab is to provide a gentle introduction to Ansible and provide an environment for further learning and experimentation. Not all Ansible features are covered, but basics such as ad hoc command execution, playbooks, and templates are introduced.","title":"Overview"},{"location":"setup/setup/#lab-software-prerequisites","text":"The lab environment consists of Ubuntu virtual machines powered by VirtualBox and managed by Vagrant. Neither VirtualBox or Vagrant are covered in depth. They are used as part of the lab, but full understanding isn't required. Each of the following should be installed prior to the lab. Setup of prerequisites will take around 20 minutes: Git and SSH are used for downloading example code and connecting to virtual machines. Many Linux distributions and macOS provide these tools out of the box. The Windows Subsystem for Linux and Git for Windows are both good options to use Git and SSH in Windows. Git for Windows is recommended. VirtualBox is an open source virtualization product. It runs on Windows, Linux, and Mac and supports most common guest operating systems. You may need to run the installer as administrator to avoid an installation error. Vagrant is a tool for building and managing virtual machine environments. A Vagrantfile provides a configuration that builds and provisions the multiple VM's in this lab. Understanding of Vagrant isn't necessary, but being familiar with a few basic commands is helpful: vagrant up : starts and provisions the Vagrant environment vagrant ssh <hostname> : connect to virtual machine with SSH vagrant halt : stops virtual machine(s) vagrant destroy : terminate and destroy virtual machines varant help : get Vagrant help","title":"Lab Software (Prerequisites)"},{"location":"setup/setup/#getting-started","text":"Open shell and change to an appropriate directory. Clone the git repository and cd into it: $ cd ~/Downloads $ git clone <PLACEHOLDER> $ cd ansible-lab Use your favorite text editor, less, or more to inspect the Vagrantfile and bootstrap-mgmt.sh files. The Vagrantfile (initially) defines four virtual machines. web1 and web2 will be configured as web servers. lb1 will be configured as an HAProxy software load balancer to distribute traffic to each webserver. A control node called mgmt will be configured (or bootstrapped) by Vagrant to automatically preinstall Ansible, copy lab examples, and populate its /etc/hosts file. It it not necessary to understand the Vagrant files in order to do the Ansible lab.","title":"Getting Started:"},{"location":"setup/setup/#start-the-virtual-machines","text":"From the ansible-lab directory, use Vagrant to start the four virtual machines. $ vagrant up The vagrant up command will take several minutes (even on a fast internet connection) as it downloads virtual machines, boots them, and configures the virtual environment. Now is a good time to take a break. Connect to the mgmt server like this: $ vagrant ssh mgmt","title":"Start the Virtual Machines"},{"location":"setup/setup/#hello-ansible","text":"If all went well, you should be at a shell prompt inside the mgmt virtual machine that looks like vagrant@mgmt:~$ . Once ssh'ed into the mgmt node. Verify that Ansible has been automatically been installed. $ ansible --version Since Ansible will be using SSH to connect to our managed nodes, make sure you can SSH to web1 . $ ssh web1 You should receive a warning similar to the following: If you know SSH, you'll recognize that the SSH client doesn't recognize the fingerprint of the SSH server. Since it is the first time connecting to this node it is to be expected. Answer yes to add web1 to the list of known hosts. The default password for the vagrant user on the lab machines is vagrant . Enter the password and you should be at a vagrant@web1:~$ prompt inside the web1 virtual machine. Use the exit command to return to the mgmt machine. Now that you can SSH to web1 you are ready for the first Ansible command. As tradition dictates, our first command will print Hello Ansible . You will be asked for the SSH password after you run the following: $ ansible -a \"echo 'Hello Ansible'\" web1 --ask-pass This is an ad hoc Ansible execution that targets the web1 node and implicitly uses the command module to execute echo 'Hello Ansible . --ask-pass (which is abbreviated -k ) tells Ansible you want to provide a password interactively. If it works, you'll see: Manually accepting host keys and typing passwords interactively is no fun. You'll see in the next session how to make it less painful.","title":"Hello Ansible!"},{"location":"setup/setup/#more-resources","text":"The Ansible documentation has a nice introduction to ad hoc commands here .","title":"More resources"}]}